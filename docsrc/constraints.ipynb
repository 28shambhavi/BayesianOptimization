{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Optimization\n",
    "\n",
    "Constrained optimization refers to situations in which you must for instance maximize \"f\", a function of \"x\" and \"y\", but the solution must lie in a region where for instance \"x<y\".\n",
    "\n",
    "There are two distinct situations you may find yourself in:\n",
    "\n",
    "1. Simple, cheap constraints: in this case, you know whether or not a given solution violates your constraints **before** you even assess it. In this case, you can codify your constraints directly into the objective function and you should read **1** below.\n",
    "2. Expensive constraints - in other situations, you may not know whether or not a given solution violates your constraints until you have explicitly evaluate the objective function there - which is typically an expensive operation. In such situations, it is desirable to **learn** the constrained regions on the fly in order to avoid unnecessary expensive calls to the objective function. The way to handle these situations is described in **2. Advanced Constrained Optimization**\n",
    "\n",
    "\n",
    "# 1. Simple Constrained Optimization\n",
    "\n",
    "In situations where you know in advance whether or not a given point violates your constraints, you can normally simply code them directly into the objective function. To demonstrate this, let's start with a standard non-constrained optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best solution with no constraints is {'target': -3.0000001172573754, 'params': {'x': 2.0, 'y': 0.999657571357127}}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def black_box_function_no_constraints(x, y):\n",
    "    return -x ** 2 - (y - 1) ** 2 + 1\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'x': (2, 4), 'y': (-3, 3)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=black_box_function_no_constraints,\n",
    "    pbounds=pbounds,\n",
    "    random_state=0,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=100\n",
    ")\n",
    "\n",
    "print(f'the best solution with no constraints is {optimizer.max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's rerun this example, except with the constraint that y>x. To do this, we are simply going to return a 'bad' value from the objective function whenever this constrain is violated. What constitutes a 'bad' value is objective function specific - in general, it's a good idea for the 'bad' value you use to be similar in magnitude to the worst value that the objective function naturally has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best solution with y>x is {'target': -4.226004059132069, 'params': {'x': 2.0, 'y': 2.107250675832744}}\n"
     ]
    }
   ],
   "source": [
    "def black_box_function_with_constraints(x, y):\n",
    "    if y <= x:\n",
    "        return -10\n",
    "    else:\n",
    "        return -x ** 2 - (y - 1) ** 2 + 1\n",
    "    \n",
    "optimizer = BayesianOptimization(\n",
    "    f=black_box_function_with_constraints,\n",
    "    pbounds=pbounds,\n",
    "    random_state=0,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=100\n",
    ")\n",
    "\n",
    "print(f'the best solution with y>x is {optimizer.max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this seems to have worked pretty well; our constraints have been respected and the target value isn't even **that** much worse!\n",
    "\n",
    "In certain other cases, you may be able to reformulate your objective function such that the constraint is explicitly embedded. For instance, consider the constraint `x+y=4`. Since this implies that `y=4-x`, we could simply reformulate the objective function explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mData point [2.] is not unique. 1 duplicates registered. Continuing ...\u001b[0m\n",
      "\u001b[91mData point [2.] is not unique. 2 duplicates registered. Continuing ...\u001b[0m\n",
      "\u001b[91mData point [2.] is not unique. 3 duplicates registered. Continuing ...\u001b[0m\n",
      "\u001b[91mData point [2.] is not unique. 4 duplicates registered. Continuing ...\u001b[0m\n",
      "\u001b[91mData point [2.] is not unique. 5 duplicates registered. Continuing ...\u001b[0m\n",
      "the best solution with y=4-x is {'target': -4.0, 'params': {'x': 2.0}}\n"
     ]
    }
   ],
   "source": [
    "def surrogate_objective(x):\n",
    "    y=4-x\n",
    "    return black_box_function_no_constraints(x,y)\n",
    "\n",
    "pbounds = {'x': (2, 4)}\n",
    "# note that in general, we would have to update pbounds such that the values that x were allowed to take on\n",
    "# respected the bounds of y. In this case (4-4=0)<=y<=(4-2=2) already respect our original bounds -3<=y<=3\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=surrogate_objective,\n",
    "    pbounds=pbounds,\n",
    "    random_state=0,\n",
    "    verbose=0,\n",
    "    allow_duplicate_points=True\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=10\n",
    ")\n",
    "\n",
    "print(f'the best solution with y=4-x is {optimizer.max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: in this last example, we have set `allow_duplicate_points=True`. The reason we are getting some duplicate points in this example is probably because the space is now so constrained that the optimizer quickly hones in on only one 'interesting' point and repeatedly probes it. The default behavior in these cases is `allow_duplicate_points=False` which will raise an error when a duplicate is registered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Advanced Constrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some situations, certain regions of the solution domain may be infeasible or not allowed. In addition, you may not know whether specific parameter combinations fall into these regions until you have evaluated the constraint function at these points. In other words, checking for feasibility is as expensive, or close to as expensive as evaluating the objective function. This notebook demonstrates how you can handle these situations by modelling the constraints as a Gaussian process. This approach is based on a paper by [Gardner et. al., 2014](http://proceedings.mlr.press/v32/gardner14.pdf).\n",
    "\n",
    "Note that if the constrained regions are known/if the constraint function is cheap to evaluate, then other approaches are preferable due to the computational complexity of modelling using Gaussian processes.\n",
    "In this case, at the time of writing the best approach is to return a low number to the optimizer when it tries to evaluate these regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Simple, single constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbayes_opt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesianOptimization\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NonlinearConstraint\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import NonlinearConstraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the use of advanced constrained bayesian optimization on the examples Gardner et al. used in their paper.\n",
    "\n",
    "Define the target function ($f$ or `target_function`) we want to optimize along with a constraint function ($c$ or `constraint_function`) and constraint limit ($c^{lim}$ or `constraint_limit`). The mathematical problem we are trying to solve is\n",
    "$$\n",
    " \\max f(x, y)\n",
    "$$\n",
    "$$\n",
    " \\text{subj. to} \\: \\: c(x, y) \\leq c^{\\text{lim}}\n",
    "$$\n",
    "Note that the constraint function should have the same parameter names as the target function (i.e. in this case `x` and `y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function(x, y):\n",
    "    # Gardner is looking for the minimum, but this packages looks for maxima, thus the sign switch\n",
    "    return np.cos(2*x)*np.cos(y) + np.sin(x)\n",
    "\n",
    "def constraint_function(x, y):\n",
    "    return np.cos(x) * np.cos(y) - np.sin(x) * np.sin(y)\n",
    "\n",
    "constraint_limit = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `scipy.NonlinearConstraint` object stores the constraint configuration. Since we do not have a lower bound on our problem, provide `-np.inf` as `lb` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = NonlinearConstraint(constraint_function, -np.inf, constraint_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `BayesianOptimization` model as you would usually, providing the `ConstraintModel` instance as additional keyword argument.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounded region of parameter space\n",
    "pbounds = {'x': (0, 6), 'y': (0, 6)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=target_function,\n",
    "    constraint=constraint,\n",
    "    pbounds=pbounds,\n",
    "    verbose=0, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the optimization as usual -- the optimizer automatically estimates the probability for the constraint to be fulfilled and modifies the acquisition function accordingly. This means, that the optimizer avoids sampling points that are likely unfeasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best combination of parameters, target value and constraint function value found by the optimizer can be accessed via the property `optimizer.max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_constrained_opt(pbounds, target_function, optimizer):\n",
    "    \"\"\"\n",
    "    Plots a number of interesting contours to visualize constrained 2-dimensional optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set a few parameters\n",
    "    n_constraints = optimizer.constraint.lb.size\n",
    "    n_plots_per_row = 2+n_constraints\n",
    "\n",
    "    # Construct the subplot titles\n",
    "    if n_constraints==1:\n",
    "        c_labels = [\"constraint\"]\n",
    "    else:\n",
    "        c_labels = [f\"constraint {i+1}\" for i in range(n_constraints)]\n",
    "    labels_top = [\"target\"] + c_labels + [\"masked target\"]\n",
    "    labels_bot = [\"target estimate\"] + [c + \" estimate\" for c in c_labels] + [\"acqusition function\"]\n",
    "    labels = [labels_top, labels_bot]\n",
    "\n",
    "    # Setup the grid to plot on\n",
    "    x = np.linspace(pbounds['x'][0], pbounds['x'][1], 1000)\n",
    "    y = np.linspace(pbounds['y'][0], pbounds['y'][1], 1000)\n",
    "    xy = np.array([[x_i, y_j] for y_j in y for x_i in x])\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    # Evaluate the actual functions on the grid\n",
    "    Z = target_function(X, Y)\n",
    "    # This reshaping is a bit painful admittedly, but it's a consequence of np.meshgrid\n",
    "    C = optimizer.constraint.fun(X, Y).reshape((n_constraints,) + Z.shape).swapaxes(0, -1)\n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(2, n_plots_per_row, constrained_layout=True, figsize=(12,8))\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(n_plots_per_row):\n",
    "            axs[i, j].set_aspect(\"equal\")\n",
    "            axs[i, j].set_title(labels[i][j])\n",
    "    \n",
    "    \n",
    "    # Extract & unpack the optimization results\n",
    "    max_ = optimizer.max\n",
    "    res = optimizer.res\n",
    "    x_ = np.array([r[\"params\"]['x'] for r in res])\n",
    "    y_ = np.array([r[\"params\"]['y'] for r in res])\n",
    "    c_ = np.array([r[\"constraint\"] for r in res])\n",
    "    a_ = np.array([r[\"allowed\"] for r in res])\n",
    "\n",
    "\n",
    "    Z_est = optimizer._gp.predict(xy).reshape(Z.shape)\n",
    "    C_est = optimizer.constraint.approx(xy).reshape(Z.shape + (n_constraints,))\n",
    "    P_allowed = optimizer.constraint.predict(xy).reshape(Z.shape)\n",
    "\n",
    "    Acq = np.where(Z_est >0, Z_est * P_allowed, Z_est / (0.5 + P_allowed))\n",
    "    \n",
    "    \n",
    "    target_vbounds = np.min([Z, Z_est]), np.max([Z, Z_est])\n",
    "    constraint_vbounds = np.min([C, C_est]), np.max([C, C_est])\n",
    "\n",
    "\n",
    "    axs[0,0].contourf(X, Y, Z, cmap=plt.cm.coolwarm, vmin=target_vbounds[0], vmax=target_vbounds[1])\n",
    "    for i in range(n_constraints):\n",
    "        axs[0,1+i].contourf(X, Y, C[:,:,i], cmap=plt.cm.coolwarm, vmin=constraint_vbounds[0], vmax=constraint_vbounds[1])\n",
    "    Z_mask = Z\n",
    "\n",
    "    Z_mask[~np.squeeze(optimizer.constraint.allowed(C))] = np.nan\n",
    "    axs[0,n_plots_per_row-1].contourf(X, Y, Z_mask, cmap=plt.cm.coolwarm, vmin=target_vbounds[0], vmax=target_vbounds[1])\n",
    "\n",
    "    axs[1,0].contourf(X, Y, Z_est, cmap=plt.cm.coolwarm, vmin=target_vbounds[0], vmax=target_vbounds[1])\n",
    "    for i in range(n_constraints):\n",
    "        axs[1,1+i].contourf(X, Y, C_est[:, :, i], cmap=plt.cm.coolwarm, vmin=constraint_vbounds[0], vmax=constraint_vbounds[1])\n",
    "    axs[1,n_plots_per_row-1].contourf(X, Y, Acq, cmap=plt.cm.coolwarm, vmin=0, vmax=1)\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(n_plots_per_row):\n",
    "            axs[i,j].scatter(x_[a_], y_[a_], c='white', s=80, edgecolors='black')\n",
    "            axs[i,j].scatter(x_[~a_], y_[~a_], c='red', s=80, edgecolors='black')\n",
    "            axs[i,j].scatter(max_[\"params\"]['x'], max_[\"params\"]['y'], s=80, c='green', edgecolors='black')\n",
    "\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now visualize the constrained optimization.\n",
    "\n",
    "In the following figure you will see two rows of plots with 3 quadratic plots each. The top row contains - in order -- contour visualizations of the target function, constraint function and the target function (masked such that only areas where the constraint is fulfilled are plotted). Additionally we have visualized the points sampled by the optimizer -- the optimal point is plotted in green, allowed (but non-optimal) points in white, and disallowed points in red. The bottom row shows -- in order -- the approximation of the target function using the gaussian regressors, the approximation of the constraint function and a visualization of the acquisition function, i.e. the function that guides the next point to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_constrained_opt(pbounds, target_function, optimizer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed with another slightly different problem of the same form. Here, we place a constraint from above and below on a function value.\n",
    "$$\n",
    " \\max f(x, y)\n",
    "$$\n",
    "$$\n",
    " \\text{subj. to} \\: \\: c^{\\text{low}} \\leq c(x, y) \\leq c^{\\text{up}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function(x, y):\n",
    "    # Gardner is looking for the minimum, but this packages looks for maxima, thus the sign switch\n",
    "    return np.sin(x) + y\n",
    "\n",
    "def constraint_function(x, y):\n",
    "    return np.sin(x) * np.sin(y)\n",
    "\n",
    "# Note that the constraint limit in case of one-dimensional constraints can be both an array of shape (1,) or a number.\n",
    "constraint_lower = np.array([-0.9])\n",
    "constraint_upper = np.array([-0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = NonlinearConstraint(constraint_function, constraint_lower, constraint_upper)\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=target_function,\n",
    "    constraint=constraint,\n",
    "    pbounds=pbounds,\n",
    "    verbose=0, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_constrained_opt(pbounds, target_function, optimizer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multiple Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally, one might need to fulfill multiple constraints. In this case, simply employ a multi-dimensional surrogate constraint function, i.e., in case of `n` constraints, a function returning an array of shape `(n,)`. Similarly, the `constraint_limit` should be an array of shape `(n,)`. The problem we are solving is\n",
    "$$\n",
    " \\max f(x, y)\n",
    "$$\n",
    "$$\n",
    " \\text{subj. to} \\: \\: c_1^{\\text{low}} \\leq c_1(x, y) < c_1^{\\text{up}}\n",
    "$$\n",
    "$$\n",
    " \\text{subj. to} \\: \\: c_2^{\\text{low}} \\leq c_2(x, y) < c_2^{\\text{up}}\n",
    "$$\n",
    "$$\n",
    " \\dots\n",
    "$$\n",
    "$$\n",
    " \\text{subj. to} \\: \\: c_n^{\\text{low}} \\leq c_n(x, y) < c_n^{\\text{up}}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function(x, y):\n",
    "    # Gardner is looking for the minimum, but this packages looks for maxima, thus the sign switch\n",
    "    return np.cos(2*x)*np.cos(y) + np.sin(x)\n",
    "\n",
    "def constraint_function_2_dim(x, y):\n",
    "    return np.array([\n",
    "        - np.cos(x) * np.cos(y) + np.sin(x) * np.sin(y),\n",
    "        - np.cos(x) * np.cos(-y) + np.sin(x) * np.sin(-y)])\n",
    "\n",
    "constraint_lower = np.array([-np.inf, -np.inf])\n",
    "constraint_upper = np.array([0.6, 0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the problem is you would in the single-constraint case and run the optimization. Note that internally the optimizer assumes conditional independence between multiple constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = NonlinearConstraint(constraint_function_2_dim, constraint_lower, constraint_upper)\n",
    "optimizer = BayesianOptimization(\n",
    "    f=target_function,\n",
    "    constraint=constraint,\n",
    "    pbounds=pbounds,\n",
    "    verbose=0, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_constrained_opt(pbounds, target_function, optimizer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "49851069de08cc5bbf068d7713ecb1523f4cab708013d75e8e72826d85a7e48d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
